# NOTE: to reproduce the results from our paper, the training _must_ be run using at least 8 GPUs
# NOTE: the `coco_gt` parameter must be filled with the complete path to the [db]/coco/val.json file generated by the
# scripts/data_preparation/prepare_cityscapes.py script when pre-processing the dataset

[general]
val_interval = 5
log_interval = 5

[body]
body = resnet50
normalization_mode = syncbn
body_params = {}
bn_frozen = no

[fpn]
extra_scales = 1
out_strides = (4, 8, 16, 32, 64)

[rpn]
num_pre_nms_train = 12000
num_post_nms_train = 2000
num_pre_nms_val = 6000
num_post_nms_val = 1000
min_size = 0
fpn_min_level = 0
fpn_levels = 5

[roi]
num_samples = 512
nms_threshold = 0.5
score_threshold = 0.05
max_predictions = 100

[sem]
fpn_min_level = 0
fpn_levels = 4
pooling_size = (64, 64)
ohem = .25

[optimizer]
lr = 0.01
weight_decay = 0.0001
weight_decay_norm = no
momentum = 0.9
nesterov = yes
# obj, bbx, roi_cls, roi_bbx, roi_msk, sem
loss_weights = (1., 1., 1., 1., 1., 1.)

[scheduler]
epochs = 130
type = multistep
update_mode = batch
params = {"gamma": 0.1, "milestones": [36000, 44000]}
burn_in_steps = 200

[dataloader]
shortest_size = 1024
longest_max_size = 4096
train_batch_size = 1
val_batch_size = 2
rgb_mean = (0.41738699, 0.45732192, 0.46886091)
rgb_std = (0.25685097, 0.26509955, 0.29067996)
random_flip = yes
random_scale = (0.5, 2.0)
num_workers = 2
train_set = train
val_set = val
coco_gt =